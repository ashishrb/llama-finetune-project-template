# Optimized training configuration for 1100 samples
model:
  base_model: "unsloth/llama-3.2-2b-instruct"
  model_name: "llama-3.2-2b-corporate-qa"
  
training:
  # OPTIMIZED for 1100 samples
  batch_size: 4                    # Increased from 2
  gradient_accumulation_steps: 4   # Effective batch size of 16
  max_seq_length: 2048            # Keep same
  num_train_epochs: 4             # Slightly more epochs for small dataset
  learning_rate: 1e-4             # Reduced from 2e-4 (more stable)
  weight_decay: 0.01
  warmup_ratio: 0.15              # Increased warmup for small dataset
  lr_scheduler_type: "cosine"
  
  # LoRA configuration (optimized for small dataset)
  lora:
    r: 32                         # Increased from 64 (less overfitting risk)
    alpha: 64                     # Increased proportionally  
    target_modules: ["q_proj", "k_proj", "v_proj", "o_proj", 
                     "gate_proj", "up_proj", "down_proj"]
    dropout: 0.15                 # Increased from 0.1 (prevent overfitting)
    bias: "none"
    task_type: "CAUSAL_LM"
  
  # Optimization settings
  fp16: true
  gradient_checkpointing: true
  dataloader_num_workers: 2       # Reduced for smaller dataset
  remove_unused_columns: false
  
  # Evaluation and logging (optimized for small dataset)
  eval_steps: 25                  # More frequent evaluation
  save_steps: 50                  # More frequent saves
  logging_steps: 5                # More frequent logging
  eval_strategy: "steps"
  save_strategy: "steps"
  load_best_model_at_end: true
  metric_for_best_model: "eval_loss"
  greater_is_better: false
  
  # Early stopping (important for small datasets)
  early_stopping_patience: 5      # Increased patience
  early_stopping_threshold: 0.005 # Tighter threshold

data:
  train_split: 0.8              # 880 samples for training
  val_split: 0.15              # 165 samples for validation  
  test_split: 0.05             # 55 samples for testing
  max_length: 2048
  prompt_template: |
    <|begin_of_text|><|start_header_id|>system<|end_header_id|>
    
    {system}<|eot_id|><|start_header_id|>user<|end_header_id|>
    
    {instruction}<|eot_id|><|start_header_id|>assistant<|end_header_id|>
    
    {output}<|eot_id|>

output:
  model_dir: "./output/models"
  logs_dir: "./output/logs"
  checkpoints_dir: "./output/checkpoints"