# config/training_config.yaml
model:
  base_model: "unsloth/llama-3.2-2b-instruct"
  model_name: "llama-3.2-2b-corporate-qa"
  
training:
  # Optimized hyperparameters for Llama-3.2-2B on 1186 samples
  batch_size: 2                    # Small batch size for 2B model
  gradient_accumulation_steps: 8   # Effective batch size of 16
  max_seq_length: 2048            # Sufficient for corporate Q&A
  num_train_epochs: 3             # Conservative to avoid overfitting
  learning_rate: 2e-4             # Standard for instruction tuning
  weight_decay: 0.01
  warmup_ratio: 0.1               # 10% warmup
  lr_scheduler_type: "cosine"
  
  # LoRA configuration for efficient fine-tuning
  lora:
    r: 64                         # Higher rank for better adaptation
    alpha: 16                     # lora_alpha = r/4 is common
    target_modules: ["q_proj", "k_proj", "v_proj", "o_proj", 
                     "gate_proj", "up_proj", "down_proj"]
    dropout: 0.1
    bias: "none"
    task_type: "CAUSAL_LM"
  
  # Optimization settings
  fp16: true                      # Enable mixed precision
  gradient_checkpointing: true    # Save memory
  dataloader_num_workers: 4
  remove_unused_columns: false
  
  # Evaluation and logging
  eval_steps: 50
  save_steps: 100
  logging_steps: 10
  eval_strategy: "steps"
  save_strategy: "steps"
  load_best_model_at_end: true
  metric_for_best_model: "eval_loss"
  greater_is_better: false
  
  # Early stopping
  early_stopping_patience: 3
  early_stopping_threshold: 0.01

data:
  train_split: 0.8              # 80% for training (949 samples)
  val_split: 0.15              # 15% for validation (178 samples)
  test_split: 0.05             # 5% for testing (59 samples)
  max_length: 2048
  prompt_template: |
    <|begin_of_text|><|start_header_id|>system<|end_header_id|>
    
    {system}<|eot_id|><|start_header_id|>user<|end_header_id|>
    
    {instruction}<|eot_id|><|start_header_id|>assistant<|end_header_id|>
    
    {output}<|eot_id|>

output:
  model_dir: "./output/models"
  logs_dir: "./output/logs"
  checkpoints_dir: "./output/checkpoints"